{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": [
        "qDIlHd5Tos6C"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.11 64-bit ('nlp_research': conda)"
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python",
      "version": "3.7.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "c5d2e4152bb4fb3e251b1ff15644590f968ec63b6d2e0a2b9d56645cfc55a300"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/kabirivan/Bert_Applications/blob/main/BERT_squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fase 1: Importar dependencias"
      ],
      "metadata": {
        "id": "aAUq7duyG5J0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install sentencepiece\n",
        "#!pip install tf-models-official\n",
        "!pip install tf-models-nightly # mejor instalar la versión en desarrollo\n",
        "!pip install tf-nightly"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /Users/xavieraguas/opt/anaconda3/envs/nlp_research/lib/python3.7/site-packages (0.1.96)\n",
            "Collecting tf-models-nightly\n",
            "  Downloading tf_models_nightly-2.5.0.dev20210816-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 894 kB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 683 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /Users/xavieraguas/opt/anaconda3/envs/nlp_research/lib/python3.7/site-packages (from tf-models-nightly) (1.19.5)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 494 kB/s \n",
            "\u001b[?25hCollecting psutil>=5.4.3\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-macosx_10_9_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 599 kB/s \n",
            "\u001b[?25hCollecting matplotlib\n",
            "  Downloading matplotlib-3.4.3-cp37-cp37m-macosx_10_9_x86_64.whl (7.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2 MB 480 kB/s \n",
            "\u001b[?25hCollecting pycocotools\n",
            "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-macosx_10_13_x86_64.whl (511 kB)\n",
            "\u001b[K     |████████████████████████████████| 511 kB 530 kB/s \n",
            "\u001b[?25hCollecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 659 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /Users/xavieraguas/opt/anaconda3/envs/nlp_research/lib/python3.7/site-packages (from tf-models-nightly) (1.3.1)\n",
            "Collecting scipy>=0.19.1\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-macosx_10_9_x86_64.whl (32.6 MB)\n",
            "\u001b[K     |█████▍                          | 5.5 MB 319 kB/s eta 0:01:25"
          ]
        }
      ],
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921dd8cc-b788-4a3c-94b8-295f7edaf6cf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf"
      ],
      "outputs": [],
      "metadata": {
        "id": "eDdYqvxPHnI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tf.__version__"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.0-dev20200822'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ],
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "f9859e9f-5200-4999-ed0d-026c3f45c3cd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "\n",
        "from official.nlp import optimization\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fv38eJzSH00S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "outputs": [],
      "metadata": {
        "id": "WjzrCZJMJN1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fase 2: Preprocesado de Datos"
      ],
      "metadata": {
        "id": "iSGw1I_zHAb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d941bc-9989-4350-d331-ad08e0f2eaec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.json\",\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/vocab.txt\",\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "E_PcOeemKcYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "7XQGdnANLziM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        }
      ],
      "metadata": {
        "id": "295nG2zQMYSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4465981-dde3-46d3-c2f1-ee873caa2f32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fase 3: Construcción del modelo"
      ],
      "metadata": {
        "id": "F4939RJqHRUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capa Squad"
      ],
      "metadata": {
        "id": "lcQcFi8kOc6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] \n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "gjmLeQjiaOo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo completo"
      ],
      "metadata": {
        "id": "aQbQFKjUOeyf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    def apply_bert(self, inputs):\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                               inputs[\"input_mask\"],\n",
        "                                               inputs[\"input_type_ids\"]])\n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "outputs": [],
      "metadata": {
        "id": "KgkIFb1GMT81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fase 4: Entrenamiento"
      ],
      "metadata": {
        "id": "lBmSU2RnHV_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación de la IA"
      ],
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 2000\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JEUxomxENRoJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5Pg6EKe2daFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "outputs": [],
      "metadata": {
        "id": "eHd5MzTdNIZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "outputs": [],
      "metadata": {
        "id": "G0cvDjBm_KXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True)\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "I6kG-HpzVK7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2216, 2040, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 7017, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2001, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2565, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([111,  74,  82, 102], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([109,  71,  80, 100], dtype=int32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ],
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783e1376-0065-4189-f959-8d3f62855e25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "outputs": [],
      "metadata": {
        "id": "V-iE2QFC_KRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Último checkpoint restaurado!!\n"
          ]
        }
      ],
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f85640d4-9adf-44f9-b612-5c414e085077"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento personalizado"
      ],
      "metadata": {
        "id": "bDgEq09xIOOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Inicio del Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Guardando checkpoint para el epoch {} en el directorio {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicio del Epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 6.0621\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-1\n",
            "Epoch 1 Lote 50 Pérdida 5.8209\n",
            "Epoch 1 Lote 100 Pérdida 5.2858\n",
            "Epoch 1 Lote 150 Pérdida 4.6081\n",
            "Epoch 1 Lote 200 Pérdida 4.1292\n",
            "Epoch 1 Lote 250 Pérdida 3.7406\n",
            "Epoch 1 Lote 300 Pérdida 3.4795\n",
            "Epoch 1 Lote 350 Pérdida 3.3128\n",
            "Epoch 1 Lote 400 Pérdida 3.1128\n",
            "Epoch 1 Lote 450 Pérdida 2.9574\n",
            "Epoch 1 Lote 500 Pérdida 2.8129\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Lote 550 Pérdida 2.6840\n",
            "Epoch 1 Lote 600 Pérdida 2.6034\n",
            "Epoch 1 Lote 650 Pérdida 2.5247\n",
            "Epoch 1 Lote 700 Pérdida 2.4478\n",
            "Epoch 1 Lote 750 Pérdida 2.3795\n",
            "Epoch 1 Lote 800 Pérdida 2.3212\n",
            "Epoch 1 Lote 850 Pérdida 2.2794\n",
            "Epoch 1 Lote 900 Pérdida 2.2269\n",
            "Epoch 1 Lote 950 Pérdida 2.1805\n",
            "Epoch 1 Lote 1000 Pérdida 2.1184\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Lote 1050 Pérdida 2.0712\n",
            "Epoch 1 Lote 1100 Pérdida 2.0192\n",
            "Epoch 1 Lote 1150 Pérdida 1.9696\n",
            "Epoch 1 Lote 1200 Pérdida 1.9270\n",
            "Epoch 1 Lote 1250 Pérdida 1.9186\n",
            "Epoch 1 Lote 1300 Pérdida 1.8983\n",
            "Epoch 1 Lote 1350 Pérdida 1.8858\n",
            "Epoch 1 Lote 1400 Pérdida 1.8687\n",
            "Epoch 1 Lote 1450 Pérdida 1.8431\n",
            "Epoch 1 Lote 1500 Pérdida 1.8203\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Lote 1550 Pérdida 1.8009\n",
            "Epoch 1 Lote 1600 Pérdida 1.7833\n",
            "Epoch 1 Lote 1650 Pérdida 1.7777\n",
            "Epoch 1 Lote 1700 Pérdida 1.7699\n",
            "Epoch 1 Lote 1750 Pérdida 1.7521\n",
            "Epoch 1 Lote 1800 Pérdida 1.7437\n",
            "Epoch 1 Lote 1850 Pérdida 1.7187\n",
            "Epoch 1 Lote 1900 Pérdida 1.6900\n",
            "Epoch 1 Lote 1950 Pérdida 1.6774\n",
            "Tiempo total para entrenar 1 epoch: 5262.5173897743225 segs\n",
            "\n",
            "Inicio del Epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 1.1600\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-5\n",
            "Epoch 2 Lote 50 Pérdida 1.1323\n",
            "Epoch 2 Lote 100 Pérdida 1.1650\n",
            "Epoch 2 Lote 150 Pérdida 1.0600\n",
            "Epoch 2 Lote 200 Pérdida 1.0460\n",
            "Epoch 2 Lote 250 Pérdida 0.9981\n",
            "Epoch 2 Lote 300 Pérdida 0.9548\n",
            "Epoch 2 Lote 350 Pérdida 0.9416\n",
            "Epoch 2 Lote 400 Pérdida 0.8956\n",
            "Epoch 2 Lote 450 Pérdida 0.8426\n",
            "Epoch 2 Lote 500 Pérdida 0.8125\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-6\n",
            "Epoch 2 Lote 550 Pérdida 0.7875\n",
            "Epoch 2 Lote 600 Pérdida 0.7739\n",
            "Epoch 2 Lote 650 Pérdida 0.7629\n",
            "Epoch 2 Lote 700 Pérdida 0.7346\n",
            "Epoch 2 Lote 750 Pérdida 0.7113\n",
            "Epoch 2 Lote 800 Pérdida 0.6930\n",
            "Epoch 2 Lote 850 Pérdida 0.6815\n",
            "Epoch 2 Lote 900 Pérdida 0.6704\n",
            "Epoch 2 Lote 950 Pérdida 0.6603\n",
            "Epoch 2 Lote 1000 Pérdida 0.6411\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Lote 1050 Pérdida 0.6289\n",
            "Epoch 2 Lote 1100 Pérdida 0.6145\n",
            "Epoch 2 Lote 1150 Pérdida 0.6004\n",
            "Epoch 2 Lote 1200 Pérdida 0.5895\n",
            "Epoch 2 Lote 1250 Pérdida 0.5867\n",
            "Epoch 2 Lote 1300 Pérdida 0.5894\n",
            "Epoch 2 Lote 1350 Pérdida 0.6004\n",
            "Epoch 2 Lote 1400 Pérdida 0.5978\n",
            "Epoch 2 Lote 1450 Pérdida 0.5938\n",
            "Epoch 2 Lote 1500 Pérdida 0.5948\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Lote 1550 Pérdida 0.5932\n",
            "Epoch 2 Lote 1600 Pérdida 0.5958\n",
            "Epoch 2 Lote 1650 Pérdida 0.6041\n",
            "Epoch 2 Lote 1700 Pérdida 0.6120\n",
            "Epoch 2 Lote 1750 Pérdida 0.6117\n",
            "Epoch 2 Lote 1800 Pérdida 0.6166\n",
            "Epoch 2 Lote 1850 Pérdida 0.6114\n",
            "Epoch 2 Lote 1900 Pérdida 0.6073\n",
            "Epoch 2 Lote 1950 Pérdida 0.6167\n",
            "Tiempo total para entrenar 1 epoch: 5323.075678825378 segs\n",
            "\n",
            "Inicio del Epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 1.6725\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-9\n",
            "Epoch 3 Lote 50 Pérdida 1.1375\n",
            "Epoch 3 Lote 100 Pérdida 1.0644\n",
            "Epoch 3 Lote 150 Pérdida 1.0579\n",
            "Epoch 3 Lote 200 Pérdida 1.0375\n",
            "Epoch 3 Lote 250 Pérdida 1.0067\n",
            "Epoch 3 Lote 300 Pérdida 0.9554\n",
            "Epoch 3 Lote 350 Pérdida 0.9430\n",
            "Epoch 3 Lote 400 Pérdida 0.8927\n",
            "Epoch 3 Lote 450 Pérdida 0.8453\n",
            "Epoch 3 Lote 500 Pérdida 0.8164\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-10\n",
            "Epoch 3 Lote 550 Pérdida 0.7860\n",
            "Epoch 3 Lote 600 Pérdida 0.7754\n",
            "Epoch 3 Lote 650 Pérdida 0.7596\n",
            "Epoch 3 Lote 700 Pérdida 0.7369\n",
            "Epoch 3 Lote 750 Pérdida 0.7108\n",
            "Epoch 3 Lote 800 Pérdida 0.6945\n",
            "Epoch 3 Lote 850 Pérdida 0.6831\n",
            "Epoch 3 Lote 900 Pérdida 0.6735\n",
            "Epoch 3 Lote 950 Pérdida 0.6600\n",
            "Epoch 3 Lote 1000 Pérdida 0.6426\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-11\n",
            "Epoch 3 Lote 1050 Pérdida 0.6262\n",
            "Epoch 3 Lote 1100 Pérdida 0.6146\n",
            "Epoch 3 Lote 1150 Pérdida 0.6007\n",
            "Epoch 3 Lote 1200 Pérdida 0.5895\n",
            "Epoch 3 Lote 1250 Pérdida 0.5879\n",
            "Epoch 3 Lote 1300 Pérdida 0.5926\n",
            "Epoch 3 Lote 1350 Pérdida 0.6027\n",
            "Epoch 3 Lote 1400 Pérdida 0.6001\n",
            "Epoch 3 Lote 1450 Pérdida 0.5950\n",
            "Epoch 3 Lote 1500 Pérdida 0.5975\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-12\n",
            "Epoch 3 Lote 1550 Pérdida 0.5964\n",
            "Epoch 3 Lote 1600 Pérdida 0.5967\n",
            "Epoch 3 Lote 1650 Pérdida 0.6051\n",
            "Epoch 3 Lote 1700 Pérdida 0.6109\n",
            "Epoch 3 Lote 1750 Pérdida 0.6126\n",
            "Epoch 3 Lote 1800 Pérdida 0.6146\n",
            "Epoch 3 Lote 1850 Pérdida 0.6134\n",
            "Epoch 3 Lote 1900 Pérdida 0.6089\n",
            "Epoch 3 Lote 1950 Pérdida 0.6163\n",
            "Tiempo total para entrenar 1 epoch: 5372.474010705948 segs\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c27881e-7ebe-438e-b6dd-6830bd4bcc79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fase 5: Evaluación"
      ],
      "metadata": {
        "id": "v6WquTCiIR7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de la evaluación"
      ],
      "metadata": {
        "id": "qDIlHd5Tos6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the dev set in the session"
      ],
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VGKl84s5WrhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ],
      "metadata": {
        "id": "DAEUcZDSpYLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yCVmIgnEo83o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tokenizer for future information needs"
      ],
      "metadata": {
        "id": "C8aSLFdmp71I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "outputs": [],
      "metadata": {
        "id": "oH5exQ7KwnuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ],
      "metadata": {
        "id": "vdHudjJ_qAzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "outputs": [],
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the eval features and the writes the tf.record file"
      ],
      "metadata": {
        "id": "HLAcwCiaqHi_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mz7kGYmUwGQb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "eval_writer.close()"
      ],
      "outputs": [],
      "metadata": {
        "id": "WpZfwPEwMabx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ],
      "metadata": {
        "id": "TbKhx3zuq844"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "eUqYvG5TxctF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llevar a cabo las prediccioness"
      ],
      "metadata": {
        "id": "tRbKrFYoo8e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir un cierto tipo de colección (como un diccionario)."
      ],
      "metadata": {
        "id": "KyckEWDbrLEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "tyWOUKaDP0H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Devuelve cada elemento del lote de salida, uno por uno."
      ],
      "metadata": {
        "id": "28abKVvqrRa4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),\n",
        "            start_logits=start_logits.numpy().tolist(),\n",
        "            end_logits=end_logits.numpy().tolist())"
      ],
      "outputs": [],
      "metadata": {
        "id": "BScaA0SZQgQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos nuestras predicciones"
      ],
      "metadata": {
        "id": "JiLOOmnLre5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs  \n",
        "    unique_ids = x.pop(\"unique_ids\")\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict):\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ]
        }
      ],
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b128075d-bfc1-4df6-852e-f1020bbc404f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escribimos nuestras predicciones en un fichero JSON que funcionará con el script de evaluación."
      ],
      "metadata": {
        "id": "PjQ6kIqGriHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "esLdRf7uM3Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción casera"
      ],
      "metadata": {
        "id": "-eaIHyDIYHHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación del diccionario de entrada"
      ],
      "metadata": {
        "id": "P0F4l5h8Zdha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenamos la pregunta y el contexto, separados por `[\"SEP\"]`, tras la tokenización, tal cual como lo hicimos con el conjunto de entrenamiento.\n",
        "\n",
        "Lo importante a recordar es que queremos que nuestra respuesta empiece y termine con una palabra real. Por ejemplo, la palabra \"ecologically\" es tokenizada como `[\"ecological\", \"##ly\"]`, y si el token de fin es `[\"ecological\"]` queremos usar la palabra \"ecologically\" como palabra final (del mismo modo si el token de fin es`[\"##ly\"]`). Por eso, empezamos dividiendo nuestro contexto en palabras, y luego pasamos a tokens, recordando qué token se corresponde con qué palabra (ver la función `tokenize_context()` para más detalle)."
      ],
      "metadata": {
        "id": "IrhHzx7ycXXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Útiles varios"
      ],
      "metadata": {
        "id": "_tuNXt98Zm4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OBjGoQ_wfmml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Indica si un cadena de caracteres se corresponde con un espacio en blanco / separador o no.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "outputs": [],
      "metadata": {
        "id": "8f_fCe_hLC12"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Toma el texto y devuelve una lista de \"palabras\" separadas segun los \n",
        "    espacios en blanco / separadores anteriores.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "outputs": [],
      "metadata": {
        "id": "4ZA4TYnxLGVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Toma una lista de palabras (devueltas por whitespace_split()) y tokeniza cada\n",
        "    palabra una por una. También almacena, para cada nuevo token, la palabra original\n",
        "    del parámetro text_words.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)\n",
        "    return text_tok, tok_to_word_id"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fsfzt3GUNWQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # Convierte 1 en 0 y viceversa\n",
        "    return seg_ids"
      ],
      "outputs": [],
      "metadata": {
        "id": "c8qreqEURUOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384\n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "outputs": [],
      "metadata": {
        "id": "P2sPGXxsYUsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creación"
      ],
      "metadata": {
        "id": "QAnaCZWTZpWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "outputs": [],
      "metadata": {
        "id": "WQlM-M8rMklA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ],
      "metadata": {
        "id": "YL_cE-o0U8mx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "outputs": [],
      "metadata": {
        "id": "SeB29SCNNQ1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-v5nLMNjZufe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicción"
      ],
      "metadata": {
        "id": "dT066rMtZ65X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CcJgDa9gVShl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretación"
      ],
      "metadata": {
        "id": "rhdGlIo5Z9IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ],
      "metadata": {
        "id": "ZQJMBkVLd9wp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:]\n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "TfwBJfsSTwRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First easy interpretation:"
      ],
      "metadata": {
        "id": "te1u6iZAawYf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "outputs": [],
      "metadata": {
        "id": "lQJ8tp-1WvI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:"
      ],
      "metadata": {
        "id": "k3PHA84rarse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xiFZ2fUiRU_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]"
      ],
      "outputs": [],
      "metadata": {
        "id": "N9KEiFHPXXeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final answer:"
      ],
      "metadata": {
        "id": "kJDBL8KKa6NP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ]
        }
      ],
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff600bb-528d-4b14-f800-731aa4448e9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "dbd02f58-62e6-45c8-8ddd-b2b6989a2385"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reto Final"
      ],
      "metadata": {
        "id": "GAXMJwopGRzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_context = '''\n",
        "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
        "Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
        "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces.'''\n",
        "\n",
        "my_question = '''What are the common symptoms of the disease?'''\n",
        "\n",
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)\n",
        "\n",
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)\n",
        "\n",
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)\n",
        "\n",
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]\n",
        "\n",
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>WHAT ARE THE COMMON SYMPTOMS OF THE DISEASE?</h2>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<blockquote> \n",
              "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
              "Common symptoms include fever, cough, fatigue, <mark>shortness of breath, and loss of</mark> smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
              "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces. </blockquote>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "27b814f9-65bf-4ae0-90d7-c246b6363ba6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "_fTt1zl8Fbom"
      }
    }
  ]
}